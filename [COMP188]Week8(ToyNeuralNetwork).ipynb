{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8\n",
    "## Toy Neural Network\n",
    "\n",
    "_This code was all pretty much copied from [this guide](https://iamtrask.github.io/2015/07/12/basic-python-network/) by \"i am trask\". I also followed [this video](https://www.youtube.com/watch?v=aircAruvnKk) to understand it better._\n",
    "\n",
    "I did this during my free time and I realised that this is rather important to understand. So I decided to write part of my Week 8 journal as this.\n",
    "\n",
    "Fundamentally, how I understand the neural network now is that essentially, at it's core, it's just one giant `y = mx+b`.\n",
    "\n",
    "#### For Predictions\n",
    "\n",
    "To extract a binary prediction out of a neural network with 1 layer, we actually just:\n",
    "\n",
    "1. Pass in a matrix of features \n",
    "2. We product that matrix of features with a matrix of weights (our synapse) \n",
    "3. We apply the sigmoid function to our result to end up at the hidden layer\n",
    "4. Then we product the layer results with another matrix of weights to make our outcome\n",
    "\n",
    "Said in psuedo-code terms:\n",
    "```\n",
    "sigmoid([features]*[weights0]) = hidden_layer\n",
    "sigmoid([hidden_layer]*[weights1]) = prediction\n",
    "```\n",
    "\n",
    "#### For learning\n",
    "\n",
    "To create the architecture and features above, we:\n",
    "\n",
    "- For our weights/synapses:\n",
    "    - We randomly assign random values to a matrix of shape\n",
    "        \n",
    "        ```(layer_n, layer_n+1)```\n",
    "    - E.g.\n",
    "    \n",
    "        ```(num_of_features, num_of_nodes_in_layer0)``` \n",
    "\n",
    "- For our algorithm:\n",
    "\n",
    "\n",
    "1. Create layers by producting our layers with the last synapse before the current layer.\n",
    "    - E.g.\n",
    "    \n",
    "    ```layer_2 = [layer_1][synapse_2]```\n",
    "    \n",
    "    ```layer_n = [layer_n-1][synapse_n]```\n",
    "\n",
    "2. Compare the last layer (our predictions layer) with the actual values\n",
    "    \n",
    "    ```error = actual_y - last_layer``` \n",
    "\n",
    "3. Get that error and times it by the derivative of the sigmoid function applied onto the last layer to get how much we should shift each weight by.\n",
    "\n",
    "    ```delta = error*deriv_sigmoid(last_layer)```\n",
    "    \n",
    "    - We have to use the derivative of the sigmoid function, because we want to know the gradient of the value, this gradient is essentially how confident our algorithm is with our given features.\n",
    "    - The derivative is producted onto the error values because it is essentially controlling how much the weight should change by.\n",
    "\n",
    "4. And apply the last two steps for every layer excluding our input layer\n",
    "\n",
    "5. Apply this as many times to get the network to correct itself.\n",
    "\n",
    "That aside, let view the...\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The sigmoid function\n",
    "def sig(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 feature columns\n",
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToyNN1Layer(X, y, nodes, epochs):\n",
    "    # Create synapses as Matrices \n",
    "    synapse0 = 2*np.random.random((X.shape[1],nodes)) - 1\n",
    "    synapse1 = 2*np.random.random((nodes,1)) - 1\n",
    "    \n",
    "    # Apply as many epochs as you want\n",
    "    for i in np.arange(epochs):\n",
    "        # FORWARD PROPAGATION \n",
    "        layer0 = X\n",
    "        layer1 = sig(np.dot(layer0, synapse0))\n",
    "        layer2 = sig(np.dot(layer1, synapse1))\n",
    "        \n",
    "        # BACKWARD PROPAGATION\n",
    "        layer2_error = y - layer2\n",
    "        layer2_delta = layer2_error * sig(layer2, deriv=True)\n",
    "        \n",
    "        layer1_error = layer2_delta.dot(synapse1.T)   \n",
    "        layer1_delta = layer1_error * sig(layer1, deriv=True)\n",
    "        \n",
    "        # UPDATING WEIGHTS IN SYNAPSES\n",
    "        synapse1 += layer1.T.dot(layer2_delta)\n",
    "        synapse0 += layer0.T.dot(layer1_delta)\n",
    "    \n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07537822],\n",
       "       [0.93151652],\n",
       "       [0.93027104],\n",
       "       [0.06121874]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ToyNN1Layer(X, y, 4, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which shows essentially, how much it's confident in sways in either direction, essentially our array is of [0, 1, 1, 0].\n",
    "\n",
    "Unfortunately this neural network only takes in a training sample and spits out how much it's learnt from the training sample, but it can be easily augmented to predict values and generate an accuracy score, it isn't much of a jump."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
